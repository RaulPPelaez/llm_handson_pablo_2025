{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e563e6",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475ae01c",
   "metadata": {},
   "source": [
    "# (Q)LoRA finetuning  \n",
    "\n",
    "### Training <0.1% of the parameters  \n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_diagram.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f57aea",
   "metadata": {},
   "source": [
    "LoRA injects low rank linear layers (BA) into every linear layer in the architecture. QLoRA is just the addition of quantization to a LoRA model.\n",
    "\n",
    "Quantization allows to finetune a 7B model on a 4090 by reducing 4x the memory requirements.\n",
    "\n",
    "Quant reduces model performance, but LoRA is able to mitigate this while also finetuning the model on a downstream task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f6a741",
   "metadata": {},
   "source": [
    "## Load the base model\n",
    "\n",
    "An already finetuned model can be finetuned further, but lets stick to a so-called pretrained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68e8df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "model_name = \"google/gemma-3-1b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map=0, torch_dtype=torch.bfloat16, attn_implementation='eager'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dabe067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "¿Cuál es la capital de Australia?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "La capital de Australia es Canberra.\n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "# Inference example\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"¿Cuál es la capital de Australia?\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "output_ids = model.generate(input_ids, max_new_tokens=512)\n",
    "response = tokenizer.batch_decode(output_ids)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534aae5c",
   "metadata": {},
   "source": [
    "# Prepare a dataset\n",
    "Our example finetuning will perform a simple task using SMILES.\n",
    "For a given SMILES we will instruct the model to produce the same SMILE but reversed.\n",
    "\n",
    "I have prepared a dataset with some smiles and their reverses. So from `smile` I want to get `smile[::-1]`.\n",
    "\n",
    "Although this would be more fitting to a sequence to sequence model, we will encode this as a CausalLM task and finetune on it using next token prediction. The 1B model should be powerful enough to handle it anyhow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1c39198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>rev_smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C=CC(=O)N1C[C@@H]2CCOc3c(Cl)c(-c4c(O)cccc4F)c(...</td>\n",
       "      <td>C1]H@C[C2N)43c(cncn4c)F(c)F4cccc)O(c4c-(c)lC(c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COc1ccc2c(c1)OCc1c-2oc2ccccc2c1=O</td>\n",
       "      <td>O=1c2ccccc2co2-c1cCO)1c(c2ccc1cOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COc1ccc2c(c1)NC1=C(C(=O)OC1)C2c1cc(OC)c(OC)c(O...</td>\n",
       "      <td>1c)CO(c)CO(c)CO(cc1c2C)1CO)O=(C(C=1CN)1c(c2ccc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CC[C@@]1(O)C[C@H](OC2CC(N(C)C)C(OC3CC(O)C(OC4C...</td>\n",
       "      <td>CO)O=(C1]H@C[)O=3C2cccc)O(c2c)O=(C)O2c(c3cc(c2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COc1ccc2c(c1)OC(N)=C(C#N)C2c1cc(OC)c(OC)c(OC)c1</td>\n",
       "      <td>1c)CO(c)CO(c)CO(cc1c2C)N#C(C=)N(CO)1c(c2ccc1cOC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles  \\\n",
       "0  C=CC(=O)N1C[C@@H]2CCOc3c(Cl)c(-c4c(O)cccc4F)c(...   \n",
       "1                  COc1ccc2c(c1)OCc1c-2oc2ccccc2c1=O   \n",
       "2  COc1ccc2c(c1)NC1=C(C(=O)OC1)C2c1cc(OC)c(OC)c(O...   \n",
       "3  CC[C@@]1(O)C[C@H](OC2CC(N(C)C)C(OC3CC(O)C(OC4C...   \n",
       "4    COc1ccc2c(c1)OC(N)=C(C#N)C2c1cc(OC)c(OC)c(OC)c1   \n",
       "\n",
       "                                          rev_smiles  \n",
       "0  C1]H@C[C2N)43c(cncn4c)F(c)F4cccc)O(c4c-(c)lC(c...  \n",
       "1                  O=1c2ccccc2co2-c1cCO)1c(c2ccc1cOC  \n",
       "2  1c)CO(c)CO(c)CO(cc1c2C)1CO)O=(C(C=1CN)1c(c2ccc...  \n",
       "3  CO)O=(C1]H@C[)O=3C2cccc)O(c2c)O=(C)O2c(c3cc(c2...  \n",
       "4    1c)CO(c)CO(c)CO(cc1c2C)N#C(C=)N(CO)1c(c2ccc1cOC  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "dsp = pd.read_csv(\"small_smiles.csv\")\n",
    "dsp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60ae38f",
   "metadata": {},
   "source": [
    "### We will create a dataset with labels so that for each `[SMILE]` and `[REV_SMILE]` pair we get:  \n",
    "`[SMILE]<reversed>[REV_SMILE]<end_of_smile>`  \n",
    "The tokenizer will prepend each sample with the special token `tokenizer.bos_token` (normally `<s>`) and will place `tokenizer.eos_token` at the end. However, adding our own special separator will help the model.\n",
    "\n",
    "#### I find that 3K examples are enough for the model to understand the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62db072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 3000\n",
    "smile_separator = \"<reversed>\"\n",
    "smile_end = \"<end_of_smile>\"\n",
    "smiles = np.array([s+smile_separator+r+smile_end for s,r in zip(dsp[\"smiles\"][:subset], dsp[\"rev_smiles\"][:subset])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f807661d",
   "metadata": {},
   "source": [
    "The tokenizer of a model intended for conversation is not really fit for SMILES, so they will tend to be encoded as a really long series of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5df0e3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "token_counts = np.array([len(tokenizer.encode(text)) for text in smiles])\n",
    "plt.hist(token_counts, bins='auto', color='green', alpha=0.7)\n",
    "plt.title('Histogram of Token Counts')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4233b7",
   "metadata": {},
   "source": [
    "### Lets filter entries with a token count larger than 512 so we can increase the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ce4651a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed entries: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C=CC(=O)N1C[C@@H]2CCOc3c(Cl)c(-c4c(O)cccc4F)c(F)c4ncnc(c34)N2C[C@H]1C&lt;reversed&gt;C1]H@C[C2N)43c(cn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COc1ccc2c(c1)OCc1c-2oc2ccccc2c1=O&lt;reversed&gt;O=1c2ccccc2co2-c1cCO)1c(c2ccc1cOC&lt;end_of_smile&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COc1ccc2c(c1)NC1=C(C(=O)OC1)C2c1cc(OC)c(OC)c(OC)c1&lt;reversed&gt;1c)CO(c)CO(c)CO(cc1c2C)1CO)O=(C(C=1C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CC[C@@]1(O)C[C@H](OC2CC(N(C)C)C(OC3CC(O)C(OC4CCC(=O)C(C)O4)C(C)O3)C(C)O2)c2c(cc3c(c2O)C(=O)c2c(O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COc1ccc2c(c1)OC(N)=C(C#N)C2c1cc(OC)c(OC)c(OC)c1&lt;reversed&gt;1c)CO(c)CO(c)CO(cc1c2C)N#C(C=)N(CO)1c(c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                labels\n",
       "0  C=CC(=O)N1C[C@@H]2CCOc3c(Cl)c(-c4c(O)cccc4F)c(F)c4ncnc(c34)N2C[C@H]1C<reversed>C1]H@C[C2N)43c(cn...\n",
       "1           COc1ccc2c(c1)OCc1c-2oc2ccccc2c1=O<reversed>O=1c2ccccc2co2-c1cCO)1c(c2ccc1cOC<end_of_smile>\n",
       "2  COc1ccc2c(c1)NC1=C(C(=O)OC1)C2c1cc(OC)c(OC)c(OC)c1<reversed>1c)CO(c)CO(c)CO(cc1c2C)1CO)O=(C(C=1C...\n",
       "3  CC[C@@]1(O)C[C@H](OC2CC(N(C)C)C(OC3CC(O)C(OC4CCC(=O)C(C)O4)C(C)O3)C(C)O2)c2c(cc3c(c2O)C(=O)c2c(O...\n",
       "4  COc1ccc2c(c1)OC(N)=C(C#N)C2c1cc(OC)c(OC)c(OC)c1<reversed>1c)CO(c)CO(c)CO(cc1c2C)N#C(C=)N(CO)1c(c..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_smiles = smiles[token_counts < 512]\n",
    "print(f\"Number of removed entries: {(token_counts>=512).sum()}\")\n",
    "ds = pd.DataFrame.from_dict({\"labels\": filtered_smiles})\n",
    "with pd.option_context('display.max_colwidth', 100):\n",
    "   display(ds.head())\n",
    "ds = datasets.Dataset.from_pandas(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2430057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['labels'],\n",
       "     num_rows: 2396\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['labels'],\n",
       "     num_rows: 300\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['labels'],\n",
       "     num_rows: 300\n",
       " })}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test = ds.train_test_split(test_size=0.2)\n",
    "validation = train_test['test'].train_test_split(test_size=0.5)\n",
    "split_ds = {\n",
    "        'train': train_test['train'],\n",
    "        'validation': validation['train'],\n",
    "        'test': validation['test']\n",
    "    }\n",
    "split_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbb1cb8",
   "metadata": {},
   "source": [
    "--------------------------------------\n",
    "# Adding a LoRA adapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "009d0f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 8-bit optimizer is not available on your device, only available on CUDA for now.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 745,472 || all params: 1,000,631,424 || trainable%: 0.0745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import peft # Parameter Efficient Finetuning. Contains utilities for adding adapters to HF models\n",
    "lora_config = peft.LoraConfig(peft_type=peft.TaskType.CAUSAL_LM,\n",
    "                              inference_mode=False,\n",
    "                              r=8, # Rank\n",
    "                              lora_alpha=32,\n",
    "                              lora_dropout=0.1)\n",
    "model = peft.get_peft_model(model, lora_config)\n",
    "display(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037fd697",
   "metadata": {},
   "source": [
    "### Inspecting the model reveals that LoRA has been hooked into some of the linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af237e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModel(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Gemma3ForCausalLM(\n",
      "      (model): Gemma3TextModel(\n",
      "        (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-25): 26 x Gemma3DecoderLayer(\n",
      "            (self_attn): Gemma3Attention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1152, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1152, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1152, out_features=256, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1152, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): Linear(in_features=1024, out_features=1152, bias=False)\n",
      "              (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "              (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "            )\n",
      "            (mlp): Gemma3MLP(\n",
      "              (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
      "              (up_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
      "              (down_proj): Linear(in_features=6912, out_features=1152, bias=False)\n",
      "              (act_fn): PytorchGELUTanh()\n",
      "            )\n",
      "            (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "            (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "            (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "            (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "        (rotary_emb): Gemma3RotaryEmbedding()\n",
      "        (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1152, out_features=262144, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed07171a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ed75e36b3d478a9cd5eb08f08380eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2396 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4db39b1c56d456abae2840ce0252c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2396 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "# Available optimizers for MPS:\n",
    "available_optimizers = [\n",
    "    \"adamw_torch\"\n",
    "]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=available_optimizers[0],\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=100,\n",
    "    logging_dir=\"./logs\",\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=4e-3,\n",
    "    max_grad_norm=0.5,\n",
    "    max_steps=500,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "# keep tokenizer padding settings\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Pre-tokenize the dataset and create labels for causal LM (labels = input_ids)\n",
    "def tokenize_fn(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"labels\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    tokenized[\"labels\"] = [ids.copy() for ids in tokenized[\"input_ids\"]]\n",
    "    return tokenized\n",
    "\n",
    "tokenized_train = split_ds[\"train\"].map(tokenize_fn, batched=True, remove_columns=[\"labels\"])\n",
    "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cefc24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1}.\n",
      "/Users/raul/miniforge3/envs/ollama/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 11:15, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.590300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.385500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.372800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.296200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.283600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.38568835067749024, metrics={'train_runtime': 678.4203, 'train_samples_per_second': 1.474, 'train_steps_per_second': 0.737, 'total_flos': 2146226798592000.0, 'train_loss': 0.38568835067749024})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8661d4",
   "metadata": {},
   "source": [
    "# Testing the model\n",
    "\n",
    "Lets try and see how the model behaves for a task by picking a SMILE and asking for its reverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0f8e670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**SMILE:** `CC1(C)CC(=O)c2c(C(F)(F)F)nn(-c3ccc(C(N)=O)c(NC4CCC(OC(=O)CN)CC4)c3)c2C1`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reverse:** `1C2c)3c)4CC)NC)O=(CO(CCC4CN(c)O=)N(C(ccc3c-(nn)F)F()F(C(c2c)O=(CC)C(1CC`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model input:**`CC1(C)CC(=O)c2c(C(F)(F)F)nn(-c3ccc(C(N)=O)c(NC4CCC(OC(=O)CN)CC4)c3)c2C1<reversed>`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "i=1 # An arbitrary sample from the test set\n",
    "smile = [s.split(smile_end)[0] for s in split_ds[\"test\"][i][\"labels\"].split(smile_separator)] \n",
    "tokens = tokenizer(smile[0]+smile_separator, return_tensors=\"pt\").to(\"mps\")\n",
    "display(Markdown(f\"**SMILE:** `{smile[0]}`\"))\n",
    "display(Markdown(f\"**Reverse:** `{smile[1]}`\"))\n",
    "display(Markdown(f\"**Model input:**`{smile[0]+smile_separator}`\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b8c945",
   "metadata": {},
   "source": [
    "## First with the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ced2c06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "### **Generated**\n",
       "------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```N1C**\n",
       "Okay, let's break down this SMILE (Structural Model Indicator Language) notation. It describes a molecule with a complex arrangement of atoms.  Here's a detailed explanation:\n",
       "\n",
       "**1.  CC1(C)CC(=O)c2c(C(F)(F)F)nn(-c```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "### **Truth**\n",
       "------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```1C2c)3c)4CC)NC)O=(CO(CCC4CN(c)O=)N(C(ccc3c-(nn)F)F()F(C(c2c)O=(CC)C(1CC```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with model.disable_adapter():\n",
    "   gen_tokens = model.generate(**tokens, max_new_tokens=len(smile[1]))[0]\n",
    "generated = tokenizer.decode(gen_tokens[tokens[\"input_ids\"].shape[1]:])\n",
    "display(Markdown(\"---\\n### **Generated**\\n------\"))\n",
    "display(Markdown(f\"```{generated.split(smile_end)[0]}```\"))\n",
    "display(Markdown(\"---\\n### **Truth**\\n------\"))\n",
    "display(Markdown(f\"```{smile[1]}```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd9cd5",
   "metadata": {},
   "source": [
    "## Now with the finetuned one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0f1efab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### **Generated**\n",
       "------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```12c)43c)O=(CN(c)2c)F)F(C=C)F)4OCO)F)F()F)F()F)F(CC(nc2c1NCC(c(c3c(cn)F(c)F(cc3cccc3cC)l```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### **Truth**\n",
       "------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```1C2c)3c)4CC)NC)O=(CO(CCC4CN(c)O=)N(C(ccc3c-(nn)F)F()F(C(c2c)O=(CC)C(1CC```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen_tokens = model.generate(**tokens, max_new_tokens=len(smile[1]))[0]\n",
    "generated = tokenizer.decode(gen_tokens[tokens[\"input_ids\"].shape[1]:])\n",
    "display(Markdown(\"### **Generated**\\n------\"))\n",
    "display(Markdown(f\"```{generated.split(smile_end)[0]}```\"))\n",
    "display(Markdown(\"### **Truth**\\n------\"))\n",
    "display(Markdown(f\"```{smile[1]}```\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cadb6a6",
   "metadata": {},
   "source": [
    "# Saving the adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7399c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"adapters/gemma_smiles_rev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c15a24c",
   "metadata": {},
   "source": [
    "### The information required to take this 15GB model from nonsense to SMILE reverser is just 14MB! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cc7d69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 6208\n",
      "  16 -rw-r--r--@ 1 raul  staff   5.1K Aug 14 19:31 README.md\n",
      "6184 -rw-r--r--@ 1 raul  staff   2.9M Aug 14 19:31 adapter_model.safetensors\n",
      "   8 -rw-r--r--@ 1 raul  staff   959B Aug 14 19:31 adapter_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls -lsrht adapters/gemma_smiles_rev"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
